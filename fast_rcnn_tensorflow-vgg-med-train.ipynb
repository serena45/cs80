{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from matplotlib import patches\n",
    "from random import shuffle, randint\n",
    "\n",
    "import selectivesearch\n",
    "import skimage.data\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allows me to import other IPython notebooks\n",
    "import ipython_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing IPython notebook from roi_pooling_importer.ipynb\n",
      "importing IPython notebook from VOC_import.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import a notebook that imports the ROI Pooling op\n",
    "from roi_pooling_importer import import_roi_pooling_op\n",
    "\n",
    "# Import a notebook that handles loading the PASCAL VOC dataset\n",
    "import VOC_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a handle to the pooling op. Since it's custom, we need to specify\n",
    "# where it's been compiled to\n",
    "roi_pooling_op_dir = os.getenv(\"HOME\") + \"/packages/tensorflow/bazel-bin/tensorflow/core/user_ops/\"\n",
    "roi_pooling_op = import_roi_pooling_op(roi_pooling_op_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This network will be trained with BGR images with pixels in range [-255, 255]\n",
    "# Takes a list of ROIs. Currently only handles one image at a time. \n",
    "# ROIs must be scaled with roi_scaling factor before input\n",
    "\n",
    "# The width and height of the image\n",
    "image_size = 224 # Must be divisible by the pooling layers\n",
    "\n",
    "# Image depth\n",
    "image_depth = 3\n",
    "\n",
    "# The batch size\n",
    "batch_size = 1\n",
    "\n",
    "# number of classes. PASCAL VOC has 20\n",
    "num_classes = 20\n",
    "num_classes_with_background = num_classes + 1\n",
    "\n",
    "# The scaling factor for ROIs - equivalent to the combined downsampling of all prior strided accesses\n",
    "# 4 (conv1) * 2 (pool1) * 2 (pool2) = 16\n",
    "roi_scaling = 16\n",
    "\n",
    "# Pixel means (probably for PASCAL VOC?) in BGR order\n",
    "pixel_means = np.array([[[102.9801, 115.9465, 122.7717]]])\n",
    "\n",
    "# Device to use\n",
    "device = \"gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters to think about tweaking:\n",
    "# LRU: k, alpha, beta\n",
    "# Inputs: input image normalization, background examples, etc\n",
    "# Data ordering: NCHW vs NHWC\n",
    "# Outputs: add bounding box regression\n",
    "# Weights: initial values\n",
    "# Dropout, weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for defining networks\n",
    "def weight_variable(shape, wd):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    weight_decay = tf.mul(tf.nn.l2_loss(initial), wd)\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x, kernel_size, stride):\n",
    "    return tf.nn.max_pool(x, ksize=[1, kernel_size, kernel_size, 1],\n",
    "                        strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def local_response_normalization(x):\n",
    "    # TODO is 5 the correct radius?\n",
    "    return tf.nn.local_response_normalization(x, depth_radius=5, alpha=.0001, beta=.75, bias=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Something like Fast-RCNN\n",
    "\n",
    "# Get a \"session\" in which we can build and run a network\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(\n",
    "    allow_soft_placement=True, log_device_placement=True))\n",
    "\n",
    "# Specify which device to run this on\n",
    "with tf.device(\"/{}:0\".format(device)):\n",
    "    \n",
    "    # Input tensors to the network. \n",
    "   \n",
    "    # Input image, in NHWC ordering\n",
    "    input_data_tensor = tf.placeholder(tf.float32, shape=[None, image_size, image_size, image_depth])\n",
    "    \n",
    "    # Input ROIs. Of shape [num_rois, 4 (x, y, height, width)]\n",
    "    rois_in = tf.placeholder(tf.int32, shape=[None, 4])\n",
    "    rois = tf.reshape(rois_in, [1, -1, 4])\n",
    "    \n",
    "    # The actual classes for each ROI. A list of length num_rois containing the\n",
    "    # index of the correct class for that ROI\n",
    "    y_ = tf.placeholder(tf.int32, shape=[None])\n",
    "    \n",
    "\n",
    "    # First Convolutional Layer, with local response normalization and max pooling\n",
    "    # Variables\n",
    "    W_conv1 = weight_variable([7, 7, image_depth, 96], wd=0.0) # filter size, filter size, input channels (image depth), output channels\n",
    "    b_conv1 = bias_variable([96])\n",
    "    # Layers\n",
    "    conv1 = conv2d(input_data_tensor, W_conv1, stride=2)\n",
    "    relu1 = tf.nn.relu(conv1 + b_conv1)\n",
    "    norm1 = local_response_normalization(relu1)\n",
    "    pool1 = max_pool(norm1, kernel_size=3, stride=2)\n",
    "\n",
    "    # Second Convolutional Layer, with local response normalization and max pooling\n",
    "    # Variables\n",
    "    W_conv2 = weight_variable([5, 5, 96, 256], wd=0.0) # filter size, filter size, input channels (image depth), output channels\n",
    "    b_conv2 = bias_variable([256])\n",
    "    # Layers\n",
    "    conv2 = conv2d(pool1, W_conv2, stride=2)\n",
    "    relu2 = tf.nn.relu(conv2 + b_conv2)\n",
    "    # error was here\n",
    "    norm2 = local_response_normalization(relu2)\n",
    "    pool2 = max_pool(norm2, kernel_size=3, stride=2)\n",
    "\n",
    "    # Third Convolutional Layer\n",
    "    # Variables\n",
    "    W_conv3 = weight_variable([3, 3, 256, 512], wd=0.0) # filter size, filter size, input channels (image depth), output channels\n",
    "    b_conv3 = bias_variable([512])\n",
    "    # Layers\n",
    "    conv3 = conv2d(pool2, W_conv3, stride=1)\n",
    "    relu3 = tf.nn.relu(conv3 + b_conv3)\n",
    "\n",
    "    # Fourth Convolutional Layer\n",
    "    # Variables\n",
    "    # Variables\n",
    "    W_conv4 = weight_variable([3, 3, 512, 512], wd=0.0) # filter size, filter size, input channels (image depth), output channels\n",
    "    b_conv4 = bias_variable([512])\n",
    "    # Layers\n",
    "    conv4 = conv2d(relu3, W_conv4, stride=1)\n",
    "    relu4 = tf.nn.relu(conv4 + b_conv4)\n",
    "\n",
    "    # Fifth Convolutional Layer\n",
    "    # Variables\n",
    "    W_conv5 = weight_variable([3, 3, 512, 512], wd=0.0) # filter size, filter size, input channels (image depth), output channels\n",
    "    b_conv5 = bias_variable([512])\n",
    "    # Layers\n",
    "    conv5 = conv2d(relu4, W_conv5, stride=1)\n",
    "    relu5 = tf.nn.relu(conv5 + b_conv5)\n",
    "\n",
    "    # ROI pooling to an output feature map of 6x6\n",
    "    # First convert NHWC to NCHW\n",
    "    relu5_transpose = tf.transpose(relu5, [0, 3, 1, 2])\n",
    "    output_dim_tensor = tf.constant((6,6))\n",
    "    roi_pool5, argmax = roi_pooling_op(relu5_transpose, rois, output_dim_tensor)\n",
    "\n",
    "    # ROI pooling outputs in NCRHW. It shouldn't matter, but let's transpose to NRCHW.\n",
    "    roi_pool5_transpose = tf.transpose(roi_pool5, [0, 2, 1, 3, 4])\n",
    "    \n",
    "    # We need to bring this down to 4-d - collapse the ROI and batch together.\n",
    "    # Should be redundant with next reshape, but whatever\n",
    "    roi_pool5_reshaped = tf.reshape(roi_pool5_transpose, (-1, 512, 6, 6))\n",
    "\n",
    "    # Fully Connected 1\n",
    "    # Weights\n",
    "    # Weight variable is sized to match num_rois * roi_pooling_feature_length (6 x 6)\n",
    "    W_fc6 = weight_variable([512 * 6 * 6, 4096], wd=0.0)\n",
    "    b_fc6 = bias_variable([4096])\n",
    "    # Layers\n",
    "    roi_pool5_flat = tf.reshape(roi_pool5_reshaped, [-1, 6 * 6 * 512])\n",
    "    fc6 = tf.matmul(roi_pool5_flat, W_fc6)\n",
    "    relu6 = tf.nn.relu(fc6 + b_fc6)\n",
    "\n",
    "    # Could insert Dropout layer here\n",
    "\n",
    "    # Fully Connected 2\n",
    "    # Weights\n",
    "    W_fc7 = weight_variable([4096, 1024], wd=0.004)\n",
    "    b_fc7 = bias_variable([1024])\n",
    "    # Layers\n",
    "    fc7 = tf.matmul(relu6, W_fc7)\n",
    "    relu7 = tf.nn.relu(fc7 + b_fc7)\n",
    "\n",
    "    # Could insert Dropout layer here\n",
    "\n",
    "    # Classification Score. Softmax layer for output classes.\n",
    "    # Weights\n",
    "    W_cls = weight_variable([1024, num_classes_with_background], wd=0.0)\n",
    "    b_cls = bias_variable([num_classes_with_background])\n",
    "    # Layers\n",
    "    cls_score = tf.matmul(relu7, W_cls) + b_cls\n",
    "    cls_prob=tf.nn.softmax(cls_score)\n",
    "    \n",
    "    # Loss function\n",
    "    # Cross entropy. Note - inputs are unscaled (not softmax output). y_'s are indexes, not vectors\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(cls_score, y_, name=None)\n",
    "    cross_entropy_loss = tf.nn.l2_loss(cross_entropy)\n",
    "    \n",
    "    # Optimizer. Use gradient descent.\n",
    "    learning_rate_tf = tf.placeholder(tf.float32)\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_tf)\n",
    "    \n",
    "    # Minimize wrt specified variables. If var_list not specified, uses all variables.\n",
    "    variables_to_optimize = [W_cls, b_cls, W_fc7, b_fc7, W_fc6, b_fc6]\n",
    "    opt_top_op = opt.minimize(cross_entropy_loss, var_list=variables_to_optimize)\n",
    "    opt_all_op = opt.minimize(cross_entropy_loss)\n",
    "    \n",
    "    # Evaluation Metrics. Top-k-percent is the percent of ROIs that are correct\n",
    "    # in one of the top k guesses\n",
    "    num_rois_tf = tf.placeholder(tf.float32)\n",
    "    top_k = tf.nn.in_top_k(cls_score, y_, 3)\n",
    "    top_k_sum = tf.reduce_sum(tf.cast(top_k, tf.float32))\n",
    "    top_k_percent = tf.div(top_k_sum, num_rois_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the variables \n",
    "with tf.device(\"/{}:0\".format(device)):\n",
    "    sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained network like AlexNet into the convolutional layers.\n",
    "imported_weights = np.load(\"model_med.numpy\", fix_imports=True, encoding=\"bytes\").tolist()\n",
    "\n",
    "with tf.device(\"/{}:0\".format(device)):\n",
    "    assign_ops = []\n",
    "    assign_ops.append(W_conv1.assign(imported_weights[\"conv1\"][b\"weights\"]))\n",
    "    assign_ops.append(b_conv1.assign(imported_weights[\"conv1\"][b\"biases\"]))\n",
    "    assign_ops.append(W_conv2.assign(imported_weights[\"conv2\"][b\"weights\"]))\n",
    "    assign_ops.append(b_conv2.assign(imported_weights[\"conv2\"][b\"biases\"]))\n",
    "    assign_ops.append(W_conv3.assign(imported_weights[\"conv3\"][b\"weights\"]))\n",
    "    assign_ops.append(b_conv3.assign(imported_weights[\"conv3\"][b\"biases\"]))\n",
    "    assign_ops.append(W_conv4.assign(imported_weights[\"conv4\"][b\"weights\"]))\n",
    "    assign_ops.append(b_conv4.assign(imported_weights[\"conv4\"][b\"biases\"]))\n",
    "    assign_ops.append(W_conv5.assign(imported_weights[\"conv5\"][b\"weights\"]))\n",
    "    assign_ops.append(b_conv5.assign(imported_weights[\"conv5\"][b\"biases\"]))\n",
    "\n",
    "    for op in assign_ops:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the moving average of a list of numbers\n",
    "def runningMeanFast(x, N):\n",
    "    return np.convolve(x, np.ones((N,))/N)[(N-1):]\n",
    "\n",
    "# Create a generator to provide input examples\n",
    "def get_inputs(sample_type):\n",
    "    if sample_type == \"train\":\n",
    "        img, roi = next(train_sample_generator)\n",
    "    else:\n",
    "        img, roi = next(test_sample_generator)\n",
    "    \n",
    "    input_data = img.reshape(batch_size, image_size, image_size, image_depth)\n",
    "    input_rois = np.asarray(roi[0]).reshape(len(roi[0]), 4)\n",
    "    # Scale ROIs to pooled size\n",
    "    input_rois = (input_rois / roi_scaling).astype(np.int32)\n",
    "    input_classes = np.asarray(roi[1])\n",
    "    \n",
    "    return input_data, input_rois, input_classes\n",
    "\n",
    "train_sample_generator = VOC_import.get_train_sample()\n",
    "test_sample_generator = VOC_import.get_val_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hold statistics about the training\n",
    "stats = []\n",
    "\n",
    "# The number of iterations through the training\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose to train all layers, or just the fully connected (top) layers\n",
    "#opt_op = opt_all_op\n",
    "opt_op = opt_top_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Set up some plots\n",
    "fig,ax1 = plt.subplots(1,1)\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Time each pass\n",
    "clk = time.time()\n",
    "\n",
    "while(True):\n",
    "    # Some learning rate. Currently not on a schedule. \n",
    "    learning_rate = .00008\n",
    "    \n",
    "    # Get the next train sample\n",
    "    input_data, input_rois, input_classes = get_inputs(\"train\")\n",
    "    \n",
    "    # Every 100 iterations, update the plots so we can see how the training is going\n",
    "    if i % 100 == 10:\n",
    "        \n",
    "        # Print timings\n",
    "        print(\"running iteration {}. last iteration took {}\".format(i, time.time()-clk))\n",
    "        clk = time.time()\n",
    "        \n",
    "        # Update the plots\n",
    "        ax1.cla()\n",
    "        ax2.cla()\n",
    "        ax1.plot(runningMeanFast(np.asarray(stats)[:, 1], 100), \"b\")\n",
    "        ax2.plot(runningMeanFast(np.asarray(stats)[:, 0], 100), \"r\")\n",
    "        ax2.plot(runningMeanFast(np.asarray(stats)[:, 3], 100), \"green\")\n",
    "        ax1.set_ylabel('loss', color = \"b\")\n",
    "        ax2.set_ylabel(\"top_k_percent\", color=\"r\")\n",
    "        fig.canvas.draw()\n",
    "    \n",
    "    # Every 5 steps, run a sample from the test set through also. Record stats for both sets\n",
    "    elif i % 5 == 0:\n",
    "        with tf.device(\"/{}:0\".format(device)):\n",
    "            train_stats = sess.run([top_k_percent, cross_entropy_loss, opt_op], feed_dict={learning_rate_tf:learning_rate, input_data_tensor:input_data, \n",
    "                                        y_:input_classes, rois_in:input_rois, num_rois_tf:len(input_rois)})\n",
    "            \n",
    "            input_data, input_rois, input_classes = get_inputs(\"test\")\n",
    "            test_stats = sess.run([top_k_percent], feed_dict={learning_rate_tf:learning_rate, input_data_tensor:input_data, \n",
    "                                        y_:input_classes, rois_in:input_rois, num_rois_tf:len(input_rois)})\n",
    "            \n",
    "            \n",
    "            stats.append(train_stats[:-1]+[learning_rate]+test_stats)\n",
    "    \n",
    "    # Else, just train the network\n",
    "    else:\n",
    "        with tf.device(\"/{}:0\".format(device)):\n",
    "            sess.run(opt_op, feed_dict={learning_rate_tf:learning_rate, input_data_tensor:input_data, \n",
    "                                        y_:input_classes, rois_in:input_rois, num_rois_tf:len(input_rois)})\n",
    "\n",
    "    \n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save stats and weights to file for later restoring\n",
    "import pickle\n",
    "out = sess.run([W_fc6, W_fc7, b_fc6, b_fc7, W_cls, b_cls])\n",
    "\n",
    "with open(\"saved_weights_2\", \"wb\") as outfile:\n",
    "    pickle.dump(out, outfile)\n",
    "\n",
    "with open(\"train_stats_2\", \"wb\") as outfile:\n",
    "    pickle.dump(stats, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model in the native TensorFlow format\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"model_2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
